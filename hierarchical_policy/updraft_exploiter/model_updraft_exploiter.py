import torch
import torch.nn as nn
from torch.distributions import Normal
from torch.nn import Linear, LSTM

from hierarchical_policy.updraft_exploiter import params_updraft_exploiter
from hierarchical_policy.updraft_exploiter.params_updraft_exploiter import ModelParameters, LearningParameters


class UpdraftExploiterActorCritic(nn.Module):
    """ This class extends the pytorch base class nn.Module to build an actor-critic network for updraft exploitation.
        The architecture consists of one Long Short-Term Memory (LSTM) layer and one linear hidden layer with 32 neurons
        each. They are used by both the actor and the critic network. All layers use the tangens hyperbolicus as an
        activation function. Input is the position of the closes updraft relative to the vehicle. Both actor and critic
        network consist of one single linear layer. The actor puts out mean values for commanded roll angle and angle of
        attack. The critic computes the state value of the closest updraft.

    Attributes
    ----------
    _params_model: ModelParameters
        Shape of the network layers

    _params_rl: LearningParameters
        Hyperparameters for training

    lstm: LSTM
        LSTM layer with 32 neurons

    hidden_layer: Linear
        Hidden layer with 32 neurons

    out_actor: Linear
        Actor network, which puts out command for roll angle and angle of attack

    out_critic: Linear
        Critic network to calculate state value
    """

    def __init__(self):
        super(UpdraftExploiterActorCritic, self).__init__()

        # instantiate parameters
        self._params_model = params_updraft_exploiter.ModelParameters()
        self._params_rl = params_updraft_exploiter.LearningParameters()

        # set model elements
        self.lstm = nn.LSTM(input_size=self._params_model.DIM_IN, hidden_size=self._params_model.DIM_LSTM,
                            batch_first=True)
        self.hidden_layer = nn.Linear(self._params_model.DIM_LSTM, self._params_model.DIM_HIDDEN)
        self.out_actor = nn.Linear(self._params_model.DIM_HIDDEN, self._params_model.DIM_OUT)
        self.out_critic = nn.Linear(self._params_model.DIM_HIDDEN, 1)

    def actor(self, updraft_obs):
        """ Performs forward pass step for the actor.

        Parameters
        ----------
        updraft_obs : ndarray
            Position of closest local updraft, relative to vehicle

        Returns
        -------
        z : ndarray
            Output of actor network (command for roll angle and angle of attack)
        """
        # evaluate lstm
        _, (h_n, c_n) = self.lstm(updraft_obs)

        # forward lstm hidden state for t = seq_len
        x = torch.tanh(h_n[-1])

        # evaluate feedforward net
        z = self.hidden_layer(x)
        z = torch.tanh(z)
        z = self.out_actor(z)
        z = torch.tanh(z)
        return z

    def critic(self, updraft_obs):
        """ Performs forward pass step for the critic, which calculates state value.

        Parameters
        ----------
        updraft_obs : ndarray
            Position of closest local updraft, relative to vehicle

        Returns
        -------
        z : float
            State value
        """
        # evaluate lstm
        _, (h_n, c_n) = self.lstm(updraft_obs)

        # forward lstm hidden state for t = seq_len
        x = torch.tanh(h_n[-1])

        # evaluate feedforward net
        z = self.hidden_layer(x)
        z = torch.tanh(z)
        z = self.out_critic(z)
        z = torch.tanh(z)
        return z

    def act(self, rel_updraft_pos, memory=None, validation_mask=False):
        """ Calculates action (commands for roll angle and angle of attack). Actions are drawn from a normal
        distribution, whose mean values are the outputs from the actor network.

        Parameters
        ----------
        rel_updraft_pos : ndarray
            Position of the closest updraft, relatively to the glider

        memory : Memory
            Stores  updraft positions, actions and log-probabilities, if activated

        validation_mask : bool
            Deactivates random action sampling for validation

        Returns
        -------
        action : ndarray
            Sampled values for roll angle and angle of attack command
        """

        action_mean = self.actor(rel_updraft_pos)
        dist = Normal(action_mean, self._params_rl.SIGMA * (not validation_mask))
        action = dist.sample()
        action_logprob = dist.log_prob(action)

        if memory is not None:
            memory.up_pos.append(rel_updraft_pos)
            memory.actions.append(action)
            memory.logprobs.append(action_logprob)

        return action.detach()

    def evaluate(self, rel_updraft_pos, action):
        """ Evaluates critic network to get state value

        Parameters
        ----------
        rel_updraft_pos : ndarray
            Position of the closest updraft, relatively to the glider

        action : float

        Returns
        -------

        """
        action_mean = self.actor(rel_updraft_pos)

        dist = Normal(action_mean, self._params_rl.SIGMA)

        action_logprobs = dist.log_prob(action) # -> ??
        dist_entropy = dist.entropy()
        state_value = self.critic(rel_updraft_pos)

        return action_logprobs, state_value, dist_entropy
