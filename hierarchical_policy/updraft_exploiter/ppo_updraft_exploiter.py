import torch
import torch.nn as nn

from torch.utils.data import DataLoader
from torch.utils.data import TensorDataset
from torch.nn.utils.rnn import pack_padded_sequence

from hierarchical_policy.updraft_exploiter import model_updraft_exploiter, params_updraft_exploiter

# Choose device here
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# device = torch.device("cpu")

class Memory:
    """ Partly adopted from 'Minimal PyTorch Implementation of Proximal Policy Optimization' (2021) by Nikhil Barhate
        https://github.com/nikhilbarhate99/PPO-PyTorch

        Saves actions, updraft positions, logprobs, rewards and terminal flags during training

        Attributes
        ----------
        actions: list
            List of previous actions

        up_pos: list
            List of previous updraft positions

        logprobs: list
            List of previous log probabilities

        rewards: list
            List of previous rewards

        is_terminals: list
            List of previous terminal flags
    """

    def __init__(self):
        self.actions = []
        self.up_pos = []
        self.logprobs = []
        self.rewards = []
        self.is_terminals = []

    def clear_memory(self):
        del self.actions[:]
        del self.up_pos[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.is_terminals[:]


class PPO:
    """ This class implements policy proximity optimization for training the updraft exploiter.

        Attributes
        ----------
        _params_rl: LearningParameters
            Hyperparameters for learning

        policy: UpdraftExploiterActorCritic
            Actor-Critic model which represents policy

        policy_old: UpdraftExploiterActorCritic
            Policy of previous training step

        optimizer: Adam
            Set Adam as optimizer for stochastic gradient descent (SGD) algorithm

        MseLoss: MSELoss
            Use mean squared error as loss function for training
    """

    def __init__(self):

        self._params_rl = params_updraft_exploiter.LearningParameters()
        self.policy = model_updraft_exploiter.UpdraftExploiterActorCritic().to(device)

        """ Uncomment the next line and insert filename to load a saved policy """
        # self.policy.load_state_dict(torch.load("actor_critic_network_final_01-April-2020_14-58.pt"))

        self.policy_old = model_updraft_exploiter.UpdraftExploiterActorCritic().to(device)
        self.policy_old.load_state_dict(self.policy.state_dict())
        self.optimizer = torch.optim.Adam(self.policy.parameters(),
                                          lr=self._params_rl.LEARNING_RATE, betas=(0.9, 0.999))
        self.MseLoss = nn.MSELoss()

    def select_action(self, rel_updraft_pos, memory=None, validation_mask=False):
        """ Gets a batch of relative updraft positions and feeds them into the LSTM layer

        Parameters
        ----------
        rel_updraft_pos : ndarray
            Batch of relative updraft positions

        memory : Memory
            Saves actions, updraft positions, logprobs, rewards and terminal flags during training

        validation_mask :
            Deactivates random action sampling for validation
        Returns
        -------
        action : ndarray
            Sampled values for roll angle and angle of attack command
        """

        rel_updraft_pos = torch.FloatTensor(rel_updraft_pos).view(1, -1, 2).to(device)  # timesteps x updrafts x features
        action = self.policy_old.act(rel_updraft_pos, memory, validation_mask).cpu().data.numpy().flatten()
        return action

    def pad_updraft_observations(self, up_obs_lst):
        """ Pads updraft observations from memory to equalize different number of updrafts during training sequence

        Parameters
        ----------
        up_obs_lst : list
            List of updraft observations from memory

        Returns
        -------
        up_obs_padded : ndarray
            Zero padded updraft observations

        seq_length : ndarray
            Tensor which contains true length of every sequence
        """
        # init tensors for padding
        up_obs_padded = torch.zeros(len(up_obs_lst), max([len(row) for batch in up_obs_lst for row in batch]), 2)
        seq_length = torch.zeros(len(up_obs_lst), dtype=torch.int)

        # loop over batch
        k = 0
        for up_obs in up_obs_lst:
            seq_length[k] = up_obs.size(1)
            up_obs_padded[k, 0:int(seq_length[k]), :] = up_obs
            k += 1

        up_obs_padded = up_obs_padded.to(device)
        seq_length = seq_length.to(device)

        return up_obs_padded, seq_length

    def get_discounted_reward_and_return_to_go(self, memory):
        """ Calculates discounted reward for all previous steps of episode with Monte Carlo estimate

        Parameters
        ----------
        memory : Memory
            Stores  updraft positions, actions and log-probabilities

        Returns
        -------
        discounted_reward : float
            Cumulated discounted reward for whole episode

        returns_to_go : list
            Cumulated rewards for all steps of episode
        """

        # Monte Carlo estimate of rewards:
        returns_to_go = []
        discounted_reward = self.policy_old.critic(memory.up_pos[-1])

        # calculate discounted reward
        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):
            if is_terminal:
                discounted_reward = 0
            discounted_reward = reward + (self._params_rl.GAMMA * discounted_reward)
            returns_to_go.insert(0, discounted_reward)

        # Normalizing the rewards:
        returns_to_go = torch.tensor(returns_to_go).to(device)
        returns_to_go = (returns_to_go - returns_to_go.mean()) / (returns_to_go.std() + 1e-5)
        returns_to_go = returns_to_go.unsqueeze(1)

        return discounted_reward, returns_to_go

    def update(self, memory):
        """ Performs one training step for AC network. A data loader is build from five input variables. The data
            loader creates randomly shuffled mini batches which are used for K training epochs.

        Parameters
        ----------
        memory : Memory
            Stores  updraft positions, actions and log-probabilities
        """

        # Get input variables for training
        discounted_reward, returns_to_go = self.get_discounted_reward_and_return_to_go(memory)
        old_actions = torch.squeeze(torch.stack(memory.actions), 1).detach()
        old_logprobs = torch.squeeze(torch.stack(memory.logprobs), 1).detach()
        old_up_obs_padded, seq_length = self.pad_updraft_observations(memory.up_pos)

        # Build data loader, which creates shuffled minibatches for training
        train_loader = DataLoader(dataset=TensorDataset(old_actions, old_logprobs, old_up_obs_padded, seq_length,
                                                        returns_to_go),
                                  batch_size=self._params_rl.MINIBATCHSIZE, shuffle=True)

        # Optimize policy for K epochs:
        for _ in range(self._params_rl.K_EPOCH):
            for mini_batch in train_loader:
                # get sampled data in mini-batch and send them to device
                old_actions, old_logprobs, old_up_obs_padded, seq_length, returns_to_go = mini_batch

                # pack padded sequence
                old_up_obs_packed = pack_padded_sequence(old_up_obs_padded, seq_length,
                                                         batch_first=True, enforce_sorted=False)
                # evaluate old actions and values
                logprobs, state_values, dist_entropy = self.policy.evaluate(old_up_obs_packed, old_actions)

                # ppo ratio
                ratios = torch.exp(logprobs - old_logprobs)

                # surrogate loss
                advantages = returns_to_go - state_values.detach()
                surr1 = ratios * advantages
                surr2 = torch.clamp(ratios, 1 - self._params_rl.EPS_CLIP, 1 + self._params_rl.EPS_CLIP) * advantages
                loss = -torch.min(surr1, surr2).mean() + 0.5 * self.MseLoss(state_values, returns_to_go)

                # gradient step
                self.optimizer.zero_grad()
                loss.mean().backward()
                self.optimizer.step()

        # transfer weights
        self.policy_old.load_state_dict(self.policy.state_dict())