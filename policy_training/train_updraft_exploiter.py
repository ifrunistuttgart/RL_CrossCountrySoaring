import datetime
import os
import shutil
import gym
import torch
import collections
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from parameters import params_triangle_soaring, params_environment
from hierarchical_policy.updraft_exploiter import params_updraft_exploiter, evaluate_updraft_exploiter
from hierarchical_policy.updraft_exploiter.ppo_updraft_exploiter import PPO, Memory

from glider.envs.glider_env_3D import GliderEnv3D

# Choose device here
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# device = torch.device("cpu")

def create_experiment_folder(params_rl, params_agent, params_logging):
    """ Creates folder to store training results. All para

    Parameters
    ----------
    params_rl : LearningParameters
        Hyperparameters for training actor-critic model

    params_agent : AgentParameters
        Parameters for initializing and simulation agent (glider)

    params_logging : LoggingParameters
        Intervals for saving and logging

    Returns
    -------
    parameterFile : file
        File object, which links to parameter file
    """

    # create folder to store data for the experiment running
    experimentID = 1
    dirName = "{}_informed_updraft_exploiter_experiment".format(experimentID)
    while os.path.exists(dirName):
        experimentID += 1
        dirName = "{}_".format(experimentID) + dirName.split('_', 1)[1]
    os.mkdir(dirName)
    shutil.copytree(os.getcwd(), os.path.join(dirName, "Sources_unzipped"),
                    ignore=shutil.ignore_patterns('*experiment*', 'archive', 'tests', '.git*', '../rl_ccs_experiments',
                                                  '.idea', '__pycache__', 'README*'))
    os.chdir(dirName)
    shutil.make_archive("Sources", 'zip', "Sources_unzipped")
    shutil.rmtree("Sources_unzipped")
    print("Directory for running experiment no. {} created".format(experimentID))

    # save parameters to file
    parameterFile = open("parameterFile.txt", "w")
    parameterFile.write(
        format(vars(params_rl)) + "\n" +
        format(vars(params_updraft_exploiter.ModelParameters())) + "\n" +
        format(vars(params_agent)) + "\n" +
        format(vars(params_logging)) + "\n\n" +
        format(vars(params_triangle_soaring.TaskParameters())) + "\n\n" +
        format(vars(params_environment.SimulationParameters())) + "\n" +
        format(vars(params_environment.GliderParameters())) + "\n" +
        format(vars(params_environment.PhysicsParameters())) + "\n" +
        format(vars(params_environment.WindParameters())))
    parameterFile.close()

    # set up file to save average returns
    returnFile = open("returnFile_running.dat", "w")
    returnFile.write("iterations,episodes,avg_returns\n")
    returnFile.close()

    return parameterFile, returnFile


def run_updraft_exploiter_training():
    """ Main function, which controls training of updraft exploiter

    """

    # set up training
    env = gym.make('glider3D-v0', agent='updraft_exploiter')
    ppo = PPO()
    memory = Memory()

    # load parameters
    _params_rl = params_updraft_exploiter.LearningParameters()
    _params_agent = params_updraft_exploiter.AgentParameters()
    _params_logging = params_updraft_exploiter.LoggingParameters()

    # export parameter file
    parameterFile, returnFile = create_experiment_folder(_params_rl, _params_agent, _params_logging)

    # set random seed for torch, gym and numpy
    if _params_rl.SEED:
        print("Random Seed: {}".format(_params_rl.SEED))
        torch.manual_seed(_params_rl.SEED)
        env.seed(_params_rl.SEED)
        np.random.seed(_params_rl.SEED)

    # initialize variables
    returns = collections.deque(maxlen=10)
    average_returns = []
    policy_iterations = 0
    episodes = 0
    interactions = 0
    ret = 0
    env.reset()
    observation = env.get_observation()

    """ Start Training loop """
    while policy_iterations < (int(_params_rl.N_ITERATIONS)):

        # run policy_old
        action = ppo.select_action(observation, memory)
        _, reward, done, _ = env.step(action)
        observation = env.get_observation()
        ret += reward

        # store reward and done flag:
        memory.rewards.append(reward)
        memory.is_terminals.append(done)
        interactions += 1

        # stop rollout if episode is completed
        if done:
            returns.append(ret)
            episodes += 1
            ret = 0
            env.reset()
            observation = env.get_observation()

            n_mean = 10 if len(returns) >= 10 else len(returns)
            average_returns.append(np.convolve(list(returns)[-n_mean:], np.ones((n_mean,)) / n_mean, mode='valid')[0])

        # update policy every BATCHSIZE interactions
        if interactions % _params_rl.BATCHSIZE == 0:
            ppo.policy.train()
            ppo.update(memory)
            ppo.policy.eval()
            memory.clear_memory()
            interactions = 0
            policy_iterations += 1

            if len(average_returns) and policy_iterations % _params_logging.PRINT_INTERVAL == 0:
                print("# policy iteration: {}/{} \t\t avg return over last 10 episodes: {:.1f}"
                      .format(policy_iterations, int(_params_rl.N_ITERATIONS), average_returns[-1]))

                with open("returnFile_running.dat", "a+") as returnFile:
                    returnFile.write(format(policy_iterations) + "," + format(episodes) + ","
                                     + '{:.1f}'.format(average_returns[-1]) + "\n")

            if policy_iterations % _params_logging.SAVE_INTERVAL == 0:
                torch.save(ppo.policy.state_dict(),
                           "updraft_exploiter_actor_critic_iter_{}".format(policy_iterations) + ".pt")
                evaluate_updraft_exploiter.main(env, ppo, policy_iterations, _params_agent, validation_mask=True)

    """ End training loop """

    # display final results
    now = datetime.datetime.now()
    returns_to_plot = pd.read_csv('returnFile_running.dat')
    returns_to_plot.plot(x='iterations', y='avg_returns')
    plt.title("evolution of average returns")
    plt.xlabel("policy iterations (-)")
    plt.ylabel("average returns (-)")
    plt.grid(True)
    plt.savefig("average_returns_" + now.strftime("%d-%B-%Y_%H-%M") + ".png")
    plt.show()

    # save actor-critic
    torch.save(ppo.policy.state_dict(),
               "updraft_exploiter_actor_critic_final_" + now.strftime("%d-%B-%Y_%H-%M") + ".pt")

    # rename parameter file consistently
    os.rename(parameterFile.name, "parameters_" + now.strftime("%d-%B-%Y_%H-%M") + ".txt")

    # rename return file consistently
    returnFile.close()
    os.rename(returnFile.name, "average_returns_" + now.strftime("%d-%B-%Y_%H-%M") + ".dat")

    env.close()


if __name__ == '__main__':
    run_updraft_exploiter_training()