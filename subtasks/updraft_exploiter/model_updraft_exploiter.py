import torch
import torch.nn as nn
from torch.distributions import Normal

from subtasks.updraft_exploiter import params_updraft_exploiter

class ActorCritic(nn.Module):
    def __init__(self):
        super(ActorCritic, self).__init__()

        # instantiate parameters
        self._params_model = params_updraft_exploiter.ModelParameters()
        self._params_rl = params_updraft_exploiter.LearningParameters()

        # set model elements
        self.lstm           = nn.LSTM(input_size=self._params_model.DIM_IN, hidden_size=self._params_model.DIM_LSTM,
                                      batch_first=True)
        self.hidden_layer   = nn.Linear(self._params_model.DIM_LSTM, self._params_model.DIM_HIDDEN)
        self.out_actor      = nn.Linear(self._params_model.DIM_HIDDEN, self._params_model.DIM_OUT)
        self.out_critic     = nn.Linear(self._params_model.DIM_HIDDEN, 1)

    def actor(self, updraft_obs):
        # evaluate lstm
        _, (h_n, c_n) = self.lstm(updraft_obs)

        # forward lstm hidden state for t = seq_len
        x = torch.tanh(h_n[-1])

        # evaluate feedforward net
        z = self.hidden_layer(x)
        z = torch.tanh(z)
        z = self.out_actor(z)
        z = torch.tanh(z)
        return z

    def critic(self, updraft_obs):
        # evaluate lstm
        _, (h_n, c_n) = self.lstm(updraft_obs)

        # forward lstm hidden state for t = seq_len
        x = torch.tanh(h_n[-1])

        # evaluate feedforward net
        z = self.hidden_layer(x)
        z = torch.tanh(z)
        z = self.out_critic(z)
        z = torch.tanh(z)
        return z
    
    def act(self, rel_updraft_pos, memory=None, validation_mask=False):
        action_mean = self.actor(rel_updraft_pos)
        dist = Normal(action_mean, self._params_rl.SIGMA * (not validation_mask))
        action = dist.sample()
        action_logprob = dist.log_prob(action)

        if memory is not None:
            memory.up_pos.append(rel_updraft_pos)
            memory.actions.append(action)
            memory.logprobs.append(action_logprob)
        
        return action.detach()
    
    def evaluate(self, rel_updraft_pos, action):
        action_mean = self.actor(rel_updraft_pos)

        dist = Normal(action_mean, self._params_rl.SIGMA)

        action_logprobs = dist.log_prob(action)
        dist_entropy = dist.entropy()
        state_value = self.critic(rel_updraft_pos)

        return action_logprobs, state_value, dist_entropy