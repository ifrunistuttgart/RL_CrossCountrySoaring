import datetime
import time
import os
import shutil
from typing import List, Any

import gym
import torch
import collections
import torch.nn as nn
from torch.distributions import MultivariateNormal, Normal
from torch.nn import MSELoss
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from torch.optim import Adam
from torch.utils.data import DataLoader
from torch.utils.data import Dataset, TensorDataset

import evaluate_updraft_exploiter
from parameters import params_environment, params_triangle_soaring
from subtasks.updraft_exploiter import params_updraft_exploiter
from subtasks.updraft_exploiter import model_updraft_exploiter

from subtasks.updraft_exploiter.model_updraft_exploiter import UpdraftExploiterActorCritic
from subtasks.updraft_exploiter.params_updraft_exploiter import LearningParameters

# choose training hardware
#device = torch.device("cuda:0")
device = torch.device("cpu")

class Memory:
    """ Saves actions, updraft positions, logprobs, rewards and terminal flags during training

        Attributes
        ----------
        actions: list
            List of previous actions

        up_pos: list
            List of previous updraft positions

        logprobs: list
            List of previous log probabilities

        rewards: list
            List of previous rewards

        is_terminals: list
            List of previous terminal flags
    """

    def __init__(self):
        self.actions = []
        self.up_pos = []
        self.logprobs = []
        self.rewards = []
        self.is_terminals = []

    def clear_memory(self):
        del self.actions[:]
        del self.up_pos[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.is_terminals[:]


class PPO:
    """ This class implements policy proximity optimization for training the updraft exploiter.

        Attributes
        ----------
        _params_rl: LearningParameters
            Hyperparameters for learning

        policy: UpdraftExploiterActorCritic
            Actor-Critic model which represents policy

        policy_old: UpdraftExploiterActorCritic
            Policy of previous training step

        optimizer: Adam
            Set Adam as optimizer for stochastic gradient descent (SGD) algorithm

        MseLoss: MSELoss
            Use mean squared error as loss function for training
    """

    def __init__(self):

        self._params_rl = params_updraft_exploiter.LearningParameters()
        self.policy = model_updraft_exploiter.UpdraftExploiterActorCritic().to(device)

        """ Uncomment the next line and insert filename to load a saved policy """
        # self.policy.load_state_dict(torch.load("actor_critic_network_final_01-April-2020_14-58.pt"))

        self.policy_old = model_updraft_exploiter.UpdraftExploiterActorCritic().to(device)
        self.policy_old.load_state_dict(self.policy.state_dict())
        self.optimizer = torch.optim.Adam(self.policy.parameters(),
                                          lr=self._params_rl.LEARNING_RATE, betas=(0.9, 0.999))
        self.MseLoss = nn.MSELoss()

    def select_action(self, rel_updraft_pos, memory=None, validation_mask=False):
        """ Gets a batch of relative updraft positions and feeds them into the LSTM layer

        Parameters
        ----------
        rel_updraft_pos : ndarray
            Batch of relative updraft positions

        memory : Memory
            Saves actions, updraft positions, logprobs, rewards and terminal flags during training

        validation_mask :
            Deactivates random action sampling for validation
        Returns
        -------
        action : ndarray
            Sampled values for roll angle and angle of attack command
        """

        rel_updraft_pos = torch.FloatTensor(rel_updraft_pos).view(1, -1, 2).to(device)  # timesteps x updrafts x features
        action = self.policy_old.act(rel_updraft_pos, memory, validation_mask).cpu().data.numpy().flatten()
        return action

    def pad_updraft_observations(self, up_obs_lst):
        """ Pads updraft observations from memory to equalize different number of updrafts during training sequence

        Parameters
        ----------
        up_obs_lst : list
            List of updraft observations from memory

        Returns
        -------
        up_obs_padded : ndarray
            Zero padded updraft observations

        seq_length : ndarray
            Tensor which contains true length of every sequence
        """
        # init tensors for padding
        up_obs_padded = torch.zeros(len(up_obs_lst), max([len(row) for batch in up_obs_lst for row in batch]), 2)
        seq_length = torch.zeros(len(up_obs_lst), dtype=torch.int)

        # loop over batch TODO: surely can be done without looping
        k = 0
        for up_obs in up_obs_lst:
            seq_length[k] = up_obs.size(1)
            up_obs_padded[k, 0:int(seq_length[k]), :] = up_obs
            k += 1

        up_obs_padded = up_obs_padded.to(device)
        seq_length = seq_length.to(device)

        return up_obs_padded, seq_length

    def get_discounted_reward_and_return_to_go(self, memory):
        """ Calculates discounted reward for all previous steps of episode with Monte Carlo estimate

        Parameters
        ----------
        memory : Memory
            Stores  updraft positions, actions and log-probabilities

        Returns
        -------
        discounted_reward : float
            Cumulated discounted reward for whole episode

        returns_to_go : list
            Cumulated rewards for all steps of episode
        """

        # Monte Carlo estimate of rewards:
        returns_to_go = []
        discounted_reward = self.policy_old.critic(memory.up_pos[-1])

        # calculate discounted reward
        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):
            if is_terminal:
                discounted_reward = 0
            discounted_reward = reward + (self._params_rl.GAMMA * discounted_reward)
            returns_to_go.insert(0, discounted_reward)

        # Normalizing the rewards:
        returns_to_go = torch.tensor(returns_to_go).to(device)
        returns_to_go = (returns_to_go - returns_to_go.mean()) / (returns_to_go.std() + 1e-5)
        returns_to_go = returns_to_go.unsqueeze(1)

        return discounted_reward, returns_to_go

    def update(self, memory):
        """ Performs one training step for AC network. A data loader is build from five input variables. The data
            loader creates randomly shuffled mini batches which are used for K training epochs.

        Parameters
        ----------
        memory : Memory
            Stores  updraft positions, actions and log-probabilities
        """

        # Get input variables for training
        discounted_reward, returns_to_go = self.get_discounted_reward_and_return_to_go(memory)
        old_actions = torch.squeeze(torch.stack(memory.actions), 1).detach()
        old_logprobs = torch.squeeze(torch.stack(memory.logprobs), 1).detach()
        old_up_obs_padded, seq_length = self.pad_updraft_observations(memory.up_pos)

        # Build data loader, which creates shuffled minibatches for training
        train_loader = DataLoader(dataset=TensorDataset(old_actions, old_logprobs, old_up_obs_padded, seq_length,
                                                        returns_to_go),
                                  batch_size=self._params_rl.MINIBATCHSIZE, shuffle=True)

        # Optimize policy for K epochs:
        for _ in range(self._params_rl.K_EPOCH):
            for mini_batch in train_loader:
                # get sampled data in mini-batch and send them to device
                old_actions, old_logprobs, old_up_obs_padded, seq_length, returns_to_go = mini_batch

                # pack padded sequence
                old_up_obs_packed = pack_padded_sequence(old_up_obs_padded, seq_length,
                                                         batch_first=True, enforce_sorted=False)
                # evaluate old actions and values
                logprobs, state_values, dist_entropy = self.policy.evaluate(old_up_obs_packed, old_actions)

                # ppo ratio
                ratios = torch.exp(logprobs - old_logprobs)

                # surrogate loss
                advantages = returns_to_go - state_values.detach()
                surr1 = ratios * advantages
                surr2 = torch.clamp(ratios, 1 - self._params_rl.EPS_CLIP, 1 + self._params_rl.EPS_CLIP) * advantages
                loss = -torch.min(surr1, surr2).mean() + 0.5 * self.MseLoss(state_values, returns_to_go)

                # gradient step
                self.optimizer.zero_grad()
                loss.mean().backward()
                self.optimizer.step()

        # transfer weights
        self.policy_old.load_state_dict(self.policy.state_dict())

def create_experiment_folder(params_rl, params_agent, params_logging):
    """ Creates folder to store training results. All para

    Parameters
    ----------
    params_rl : LearningParameters
        Hyperparameters for training actor-critic model

    params_agent : AgentParameters
        Parameters for initializing and simulation agent (glider)

    params_logging : LoggingParameters
        Intervals for saving and logging

    Returns
    -------
    parameterFile : file
        File object, which links to parameter file
    """

    # create folder to store data for the experiment running
    experimentID = 1
    dirName = "{}_informed_updraft_exploiter_experiment".format(experimentID)
    while os.path.exists(dirName):
        experimentID += 1
        dirName = "{}_".format(experimentID) + dirName.split('_', 1)[1]
    os.mkdir(dirName)
    shutil.copytree(os.getcwd(), os.path.join(dirName, "Sources_unzipped"),
                    ignore=shutil.ignore_patterns('*experiment*', 'archive', 'tests', '.git*', 'rl_ccs_experiments',
                                                  '.idea', '__pycache__', 'README*'))
    os.chdir(dirName)
    shutil.make_archive("Sources", 'zip', "Sources_unzipped")
    shutil.rmtree("Sources_unzipped")
    print("Directory for running experiment no. {} created".format(experimentID))

    # save parameters to file
    parameterFile = open("parameterFile.txt", "w")
    parameterFile.write(
        format(vars(params_rl)) + "\n" +
        format(vars(params_updraft_exploiter.ModelParameters())) + "\n" +
        format(vars(params_agent)) + "\n" +
        format(vars(params_logging)) + "\n\n" +
        format(vars(params_triangle_soaring.TaskParameters())) + "\n\n" +
        format(vars(params_environment.SimulationParameters())) + "\n" +
        format(vars(params_environment.GliderParameters())) + "\n" +
        format(vars(params_environment.PhysicsParameters())) + "\n" +
        format(vars(params_environment.WindParameters())))
    parameterFile.close()

    # set up file to save average returns
    returnFile = open("returnFile_running.dat", "w")
    returnFile.write("iterations,episodes,avg_returns\n")
    returnFile.close()

    return parameterFile, returnFile


def run_updraft_exploiter_training():
    """ Main function, which controls training of updraft exploiter

    """

    # set up training
    env = gym.make('glider3D-v0', agent='updraft_exploiter')
    ppo = PPO()
    memory = Memory()

    # load parameters
    _params_rl = params_updraft_exploiter.LearningParameters()
    _params_agent = params_updraft_exploiter.AgentParameters()
    _params_logging = params_updraft_exploiter.LoggingParameters()

    # export parameter file
    parameterFile, returnFile = create_experiment_folder(_params_rl, _params_agent, _params_logging)

    # set random seed for torch, gym and numpy
    if _params_rl.SEED:
        print("Random Seed: {}".format(_params_rl.SEED))
        torch.manual_seed(_params_rl.SEED)
        env.seed(_params_rl.SEED)
        np.random.seed(_params_rl.SEED)

    # initialize variables
    returns = collections.deque(maxlen=10)
    average_returns = []
    policy_iterations = 0
    episodes = 0
    interactions = 0
    ret = 0
    env.reset()
    observation = env.get_observation()

    """ Start Training loop """
    while policy_iterations < (int(_params_rl.N_ITERATIONS)):

        # run policy_old
        action = ppo.select_action(observation, memory)
        _, reward, done, _ = env.step(action)
        observation = env.get_observation()
        ret += reward

        # store reward and done flag:
        memory.rewards.append(reward)
        memory.is_terminals.append(done)
        interactions += 1

        # stop rollout if episode is completed
        if done:
            returns.append(ret)
            episodes += 1
            ret = 0
            env.reset()
            observation = env.get_observation()

            n_mean = 10 if len(returns) >= 10 else len(returns)
            average_returns.append(np.convolve(list(returns)[-n_mean:], np.ones((n_mean,)) / n_mean, mode='valid')[0])

        if interactions == 10:
            ppo.policy.train()
            ppo.update(memory)
            ppo.policy.eval()
            memory.clear_memory()
            interactions = 0
            policy_iterations += 1

            if len(average_returns) and policy_iterations % _params_logging.PRINT_INTERVAL == 0:
                print("# policy iteration: {}/{} \t\t avg return over last 10 episodes: {:.1f}"
                      .format(policy_iterations, int(_params_rl.N_ITERATIONS), average_returns[-1]))

                with open("returnFile_running.dat", "a+") as returnFile:
                    returnFile.write(format(policy_iterations) + "," + format(episodes) + ","
                                     + '{:.1f}'.format(average_returns[-1]) + "\n")

            if policy_iterations % _params_logging.SAVE_INTERVAL == 0:
                torch.save(ppo.policy.state_dict(),
                           "updraft_exploiter_actor_critic_iter_{}".format(policy_iterations) + ".pt")
                evaluate_updraft_exploiter.main(env, ppo, policy_iterations, _params_agent, validation_mask=True)

    """ End training loop """

    # display final results
    now = datetime.datetime.now()
    returns_to_plot = pd.read_csv('returnFile_running.dat')
    returns_to_plot.plot(x='iterations', y='avg_returns')
    plt.title("evolution of average returns")
    plt.xlabel("policy iterations (-)")
    plt.ylabel("average returns (-)")
    plt.grid(True)
    plt.savefig("average_returns_" + now.strftime("%d-%B-%Y_%H-%M") + ".png")
    plt.show()

    # save actor-critic
    torch.save(ppo.policy.state_dict(),
               "updraft_exploiter_actor_critic_final_" + now.strftime("%d-%B-%Y_%H-%M") + ".pt")

    # rename parameter file consistently
    os.rename(parameterFile.name, "parameters_" + now.strftime("%d-%B-%Y_%H-%M") + ".txt")

    # rename return file consistently
    returnFile.close()
    os.rename(returnFile.name, "average_returns_" + now.strftime("%d-%B-%Y_%H-%M") + ".dat")

    env.close()


if __name__ == '__main__':
    run_updraft_exploiter_training()